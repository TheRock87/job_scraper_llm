name: ðŸ¤– AI Job Processing Workflow

permissions:
  contents: write

on:
  # Manual trigger
  workflow_dispatch:
    inputs:
      force_process:
        description: 'Force process all jobs (ignore new jobs only)'
        required: false
        default: false
        type: boolean
  
  # Trigger on schedule (daily at 9 AM UTC)
  schedule:
    - cron: '0 9 * * *'
  
  # Trigger when jobs_raw.csv is updated
  push:
    paths:
      - 'job_filter_app/jobs_raw.csv'
    branches:
      - main

env:
  PYTHON_VERSION: '3.11'

jobs:
  process-jobs:
    runs-on: ubuntu-latest
    
    steps:
    - name: ðŸ“¥ Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for job tracking
    
    - name: ðŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: ðŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas pyyaml requests beautifulsoup4 numpy pydantic markdownify tls-client urllib3 regex
    
    - name: "ðŸ•¸ï¸ Scrape jobs and generate jobs_raw.csv"
      run: |
        cd job_filter_app
        python scraper_only.py
    
    - name: ðŸ” Process and identify new jobs
      id: process-jobs
      run: |
        cd job_filter_app
        python github_actions_workflow.py
      env:
        CSV_FILE: jobs_raw.csv
        HISTORY_FILE: job_history.json
        OUTPUT_DIR: processed_jobs
    
    - name: ðŸ“Š Display processing results
      run: |
        echo "ðŸ“ˆ New jobs found: ${{ steps.process-jobs.outputs.new_jobs_count }}"
        echo "ðŸ“‹ Total jobs: ${{ steps.process-jobs.outputs.total_jobs_count }}"
        echo "ðŸ†• Has new jobs: ${{ steps.process-jobs.outputs.has_new_jobs }}"
    
    - name: ðŸš« Skip if no new jobs (unless forced)
      if: steps.process-jobs.outputs.has_new_jobs != 'true' && github.event.inputs.force_process != 'true'
      run: |
        echo "â­ï¸ No new jobs found, skipping LLM processing"
        exit 0
    
    - name: ðŸ“¤ Upload results as artifacts
      if: steps.process-jobs.outputs.has_new_jobs == 'true' || github.event.inputs.force_process == 'true'
      uses: actions/upload-artifact@v4
      with:
        name: job-processing-results
        path: |
          job_filter_app/processed_jobs/
          job_filter_app/job_history.json
        retention-days: 30
    
    - name: â˜ï¸ Setup rclone for Google Drive upload
      if: steps.process-jobs.outputs.has_new_jobs == 'true' || github.event.inputs.force_process == 'true'
      run: |
        # Install rclone
        curl https://rclone.org/install.sh | sudo bash
        
        # Create rclone config directory
        mkdir -p ~/.config/rclone
        
        # Create minimal rclone config (will be overridden by secrets)
        cat > ~/.config/rclone/rclone.conf << EOF
        [gdrive]
        type = drive
        scope = drive
        token = 
        EOF
    
    - name: â˜ï¸ Upload to Google Drive
      if: steps.process-jobs.outputs.has_new_jobs == 'true' || github.event.inputs.force_process == 'true'
      run: |
        cd job_filter_app
        
        # Upload processed files to Google Drive
        echo "â˜ï¸ Uploading processed results to Google Drive..."
        
        # Upload processed CSV
        if [ -f "processed_jobs/processed_jobs.csv" ]; then
          rclone copy processed_jobs/processed_jobs.csv gdrive:AI-Jobs/ --progress
          echo "âœ… Uploaded processed_jobs.csv"
        fi
        
        # Upload HTML file
        if [ -f "processed_jobs/filtered_jobs.html" ]; then
          rclone copy processed_jobs/filtered_jobs.html gdrive:AI-Jobs/ --progress
          echo "âœ… Uploaded filtered_jobs.html"
        fi
        
        # Upload summary
        if [ -f "processed_jobs/processing_summary.json" ]; then
          rclone copy processed_jobs/processing_summary.json gdrive:AI-Jobs/ --progress
          echo "âœ… Uploaded processing_summary.json"
        fi
        
        echo "ðŸŽ‰ All files uploaded to Google Drive AI-Jobs folder!"
      env:
        RCLONE_CONFIG: ${{ secrets.RCLONE_CONFIG }}
    
    - name: ðŸ“Š Create summary comment
      if: steps.process-jobs.outputs.has_new_jobs == 'true' || github.event.inputs.force_process == 'true'
      run: |
        echo "## ðŸ¤– Job Processing Complete" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **Total jobs processed:** ${{ steps.process-jobs.outputs.total_jobs_count }}" >> $GITHUB_STEP_SUMMARY
        echo "- **New jobs found:** ${{ steps.process-jobs.outputs.new_jobs_count }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Processing triggered:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "ðŸ“ **Artifacts available:**" >> $GITHUB_STEP_SUMMARY
        echo "- `processed_jobs/processed_jobs.csv` - All jobs with classifications" >> $GITHUB_STEP_SUMMARY
        echo "- `processed_jobs/filtered_jobs.html` - Interactive HTML table" >> $GITHUB_STEP_SUMMARY
        echo "- `processed_jobs/processing_summary.json` - Processing statistics" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "â˜ï¸ **Google Drive:** Files uploaded to AI-Jobs folder" >> $GITHUB_STEP_SUMMARY
    
    - name: ðŸ”„ Commit updated history
      if: steps.process-jobs.outputs.has_new_jobs == 'true' || github.event.inputs.force_process == 'true'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add job_filter_app/job_history.json
        git commit -m "ðŸ“Š Update job history - ${{ steps.process-jobs.outputs.new_jobs_count }} new jobs" || echo "No changes to commit"
        git push 